{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52f58ae-371d-440b-9146-470a9644bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "!pip install -q transformers bitsandbytes peft trl accelerate xformers wandb datasets gradio\n",
    "!pip install -q spicy\n",
    "!pip install -q torchtext\n",
    "!pip install -q nltk\n",
    "!pip install -q tqdm\n",
    "!pip install -q fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841540ef-0638-403c-8e88-7f1fed6a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import gradio\n",
    "import platform\n",
    "from huggingface_hub import notebook_login\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import tqdm\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7207e1-c361-44ef-99b3-8056327845ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of CUDA devices: 1\n",
      "--- CUDA Device 0 ---\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute Capability: (8, 6)\n",
      "Total Memory: 51040157696 bytes\n",
      "--- CPU Information ---\n",
      "Processor: x86_64\n",
      "System: Linux 5.4.0-163-generic\n",
      "Python Version: 3.10.13\n"
     ]
    }
   ],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "# Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e892e417-83b0-42fd-a884-c1700812abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre trained model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8fee06-3c7f-4d1d-9f29-9ff40c62f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshyboykt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "huggingface_hub.login(\"hf_xxxx\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"xxxx\")\n",
    "\n",
    "!mkdir -p /workspace/cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf07738d-89df-4cb5-a28d-fe357bbc83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config\n",
    "    \n",
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006f3cc1-0ae2-4041-b02a-5a75a20341b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config, model_max_length):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.model_max_length = model_max_length\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ad1acf-3655-4b80-a74f-c17c38b051b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "563f4775-2c89-43a5-8e90-043526cb3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get lora module names\n",
    "# modules = find_all_linear_names(model)\n",
    "\n",
    "# # Create PEFT config for these modules and wrap the model to PEFT\n",
    "# peft_config = create_peft_config(modules)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# # Print information about the percentage of trainable parameters\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68554081-3587-4326-8363-a47f5dd3f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear the memory footprint\n",
    "# del model, trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8f302a-cb88-4190-80ec-02c0ec9666c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire.Fire(train, command=f'--base_model={model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f9c266-4ee6-47c3-9040-ce27ecb17237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: \n",
      "data_path: ./alpaca_data_cleaned.json\n",
      "output_dir: ./lora-alpaca\n",
      "batch_size: 8\n",
      "micro_batch_size: 2\n",
      "num_epochs: 3\n",
      "learning_rate: 0.0001\n",
      "cutoff_len: 512\n",
      "val_set_size: 10\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "resume_from_checkpoint: lora-alpaca/checkpoint-1600\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a33523cb774f9f9253b19dd0e308de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_from_checkpoint='lora-alpaca/checkpoint-1600'\n",
      "start testing\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-4:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-1:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-2:\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-3:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "\n",
    "# model/data params\n",
    "base_model: str = \"\"  # the only required argument\n",
    "data_path: str = \"./alpaca_data_cleaned.json\"\n",
    "output_dir: str = \"./lora-alpaca\"\n",
    "# training hyperparams\n",
    "batch_size: int = 8\n",
    "micro_batch_size: int = 2\n",
    "num_epochs: int = 3\n",
    "learning_rate: float = 1e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 10\n",
    "# lora hyperparams\n",
    "lora_r: int = 8\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "lora_target_modules: List[str] = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False, masks out inputs in loss\n",
    "group_by_length: bool = False  # faster, but produces an odd training loss curve,\n",
    "resume_from_checkpoint: str = \"lora-alpaca/checkpoint-1600\"  # either training checkpoint or final adapter\n",
    "print(\n",
    "    f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "    f\"base_model: {base_model}\\n\"\n",
    "    f\"data_path: {data_path}\\n\"\n",
    "    f\"output_dir: {output_dir}\\n\"\n",
    "    f\"batch_size: {batch_size}\\n\"\n",
    "    f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "    f\"num_epochs: {num_epochs}\\n\"\n",
    "    f\"learning_rate: {learning_rate}\\n\"\n",
    "    f\"cutoff_len: {cutoff_len}\\n\"\n",
    "    f\"val_set_size: {val_set_size}\\n\"\n",
    "    f\"lora_r: {lora_r}\\n\"\n",
    "    f\"lora_alpha: {lora_alpha}\\n\"\n",
    "    f\"lora_dropout: {lora_dropout}\\n\"\n",
    "    f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "    f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "    f\"group_by_length: {group_by_length}\\n\"\n",
    "    f\"resume_from_checkpoint: {resume_from_checkpoint}\\n\"\n",
    ")\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{40960}MB'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# Needed for LLaMA tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.model_max_length = cutoff_len\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# # Get lora module names\n",
    "# # modules = find_all_linear_names(model)\n",
    "# modules = lora_target_modules\n",
    "# print(f\"{modules=}\")\n",
    "\n",
    "# # Create PEFT config for these modules and wrap the model to PEFT\n",
    "# peft_config = create_peft_config(modules)\n",
    "# #peft_config.inference_mode = True\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "# Print information about the percentage of trainable parameters\n",
    "# print_trainable_parameters(model)\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return data_point.replace(\"_\", \" \")\n",
    "\n",
    "def generate_and_tokenize_prompt(item):\n",
    "    data_point = item[\"content\"]\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "if resume_from_checkpoint:    \n",
    "    if os.path.exists(resume_from_checkpoint):\n",
    "        print(f\"{resume_from_checkpoint=}\")\n",
    "        model = PeftModel.from_pretrained(model, resume_from_checkpoint, torch_dtype=torch.float16, \n",
    "                    device_map={\"\": \"cpu\"})\n",
    "        model = model.merge_and_unload()\n",
    "        #model.save_pretrained(\"merge-model\")\n",
    "    else:\n",
    "        print(f\"Checkpoint {resume_from_checkpoint} not found\")\n",
    "\n",
    "print(\"start testing\")\n",
    "print(type(model))\n",
    "model.eval()\n",
    "if torch.__version__ >= \"2\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=10,\n",
    "    num_beams=2,\n",
    "    max_new_tokens=512,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    print(\"output: \", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6f7c7c-86ce-443e-8859-1230342e7126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1586/905711589.py:76: GradioDeprecationWarning: The `enable_queue` parameter has been deprecated. Please use the `.queue()` method instead.\n",
      "  demo.queue(max_size=32).launch(enable_queue=True, server_name=\"0.0.0.0\", server_port=5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TextIteratorStreamer\n",
    "\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "server_name = \"0.0.0.0\"\n",
    "server_port = 5000\n",
    "\n",
    "\n",
    "def run_generation(user_text, top_p, temperature, top_k, max_new_tokens):\n",
    "    # Get the model and tokenizer, and tokenize the user text.\n",
    "    model_inputs = tokenizer([user_text], return_tensors=\"pt\").to(torch_device)\n",
    "\n",
    "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
    "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=float(temperature),\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    # Pull the generated text from the streamer, and update the model output.\n",
    "    model_output = user_text + \" \"\n",
    "    for new_text in streamer:\n",
    "        model_output += new_text\n",
    "        yield model_output\n",
    "    return model_output\n",
    "\n",
    "\n",
    "def reset_textbox():\n",
    "    return gr.update(value='')\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"# Demo Natural Language Generation - group 5:\\n\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            user_text = gr.Textbox(\n",
    "                placeholder=\"Input text here...\",\n",
    "                label=\"User input\"\n",
    "            )\n",
    "            model_output = gr.Textbox(label=\"Model output\", lines=10, interactive=False)\n",
    "            button_submit = gr.Button(value=\"Submit\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            max_new_tokens = gr.Slider(\n",
    "                minimum=1, maximum=512, value=512, step=1, interactive=True, label=\"Max New Tokens\",\n",
    "            )\n",
    "            top_p = gr.Slider(\n",
    "                minimum=0.05, maximum=1.0, value=0.95, step=0.05, interactive=True, label=\"Top-p (nucleus sampling)\",\n",
    "            )\n",
    "            top_k = gr.Slider(\n",
    "                minimum=1, maximum=50, value=50, step=1, interactive=True, label=\"Top-k\",\n",
    "            )\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.1, maximum=5.0, value=0.8, step=0.1, interactive=True, label=\"Temperature\",\n",
    "            )\n",
    "\n",
    "    user_text.submit(run_generation, [user_text, top_p, temperature, top_k, max_new_tokens], model_output)\n",
    "    button_submit.click(run_generation, [user_text, top_p, temperature, top_k, max_new_tokens], model_output)\n",
    "\n",
    "    demo.queue(max_size=32).launch(enable_queue=True, server_name=\"0.0.0.0\", server_port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b4921b-cddb-45d7-8ed4-333f0b9978ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5000\n"
     ]
    }
   ],
   "source": [
    "#demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
